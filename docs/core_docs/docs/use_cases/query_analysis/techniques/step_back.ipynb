{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Step Back Prompting\n",
    "\n",
    "Sometimes search quality and model generations can be tripped up by the specifics of a question. One way to handle this is to first generate a more abstract, \"step back\" question and to query based on both the original and step back question.\n",
    "\n",
    "For example, if we ask a question of the form \"Why does my LangGraph agent streamEvents return {LONG_TRACE} instead of {DESIRED_OUTPUT}\" we will likely retrieve more relevant documents if we search with the more generic question \"How does streamEvents work with a LangGraph agent\" than if we search with the specific user question.\n",
    "\n",
    "Let's take a look at how we might use step back prompting in the context of our Q&A bot over the LangChain YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies\n",
    "\n",
    "```{=mdx}\n",
    "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
    "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
    "\n",
    "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
    "\n",
    "<Npm2Yarn>\n",
    "  @langchain/core @langchain/openai zod\n",
    "</Npm2Yarn>\n",
    "```\n",
    "\n",
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key\n",
    "\n",
    "# Optional, use LangSmith for best-in-class observability\n",
    "LANGSMITH_API_KEY=your-api-key\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "## Step back question generation\n",
    "\n",
    "Generating good step back questions comes down to writing a good prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const system = `You are an expert at taking a specific question and extracting a more generic question that gets at \\\n",
    "the underlying principles needed to answer the specific question.\n",
    "\n",
    "You will be asked about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
    "\n",
    "Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question. \\\n",
    "\n",
    "If you don't recognize a word or acronym to not try to rewrite it.\n",
    "\n",
    "Write concise questions.`;\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "  [\n",
    "    [\"system\", system],\n",
    "    [\"human\", \"{question}\"],\n",
    "  ]\n",
    ")\n",
    "const llm = new ChatOpenAI({\n",
    "modelName: \"gpt-3.5-turbo-0125\",\n",
    "temperature: 0\n",
    "});\n",
    "const stepBack = prompt.pipe(llm).pipe(new StringOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the specific methods or functions within LangGraph that allow for filtering or extracting LLM calls from an event stream?\n"
     ]
    }
   ],
   "source": [
    "const question = `I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search.\n",
    "How do I get just the LLM calls from the event stream`\n",
    "const result = await stepBack.invoke({\"question\": question})\n",
    "console.log(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58a714-9368-4e8e-a163-58bc4a5e56e6",
   "metadata": {},
   "source": [
    "## Returning the stepback question and the original question\n",
    "\n",
    "To increase our recall we'll likely want to retrieve documents based on both the step back question and the original question. We can easily return both like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447ed63c-ba9f-4eaf-8ed8-b3235e45da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  question: \u001b[32m\"I built a LangGraph agent using Gemini Pro and tools like vectorstores and duckduckgo search.\\n\"\u001b[39m +\n",
       "    \u001b[32m\"How do\"\u001b[39m... 47 more characters,\n",
       "  stepBack: \u001b[32m\"What is the process for extracting specific types of calls, such as LLM calls, from an event stream \"\u001b[39m... 37 more characters\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "\n",
    "const stepBackAndOriginal = RunnablePassthrough.assign({ stepBack })\n",
    "\n",
    "await stepBackAndOriginal.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ed79f-cb6f-490e-8186-a4a05b223857",
   "metadata": {},
   "source": [
    "## Using function-calling to get structured output\n",
    "\n",
    "If we were composing this technique with other query analysis techniques, we'd likely be using function calling to get out structured query objects. We can use function-calling for step back prompting like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1fecf6-9c07-4efa-80eb-8fb15392b25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  stepBackQuestion: \u001b[32m\"What are the steps involved in extracting specific types of calls from an event stream in a software\"\u001b[39m... 13 more characters\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "\n",
    "const stepBackQuerySchema = z.object({\n",
    "    stepBackQuestion: z.string().describe(\"Given a specific user question about one or more of these products, write a more generic question that needs to be answered in order to answer the specific question.\")\n",
    "});\n",
    "\n",
    "const llmWithTools = llm.withStructuredOutput(stepBackQuerySchema, {\n",
    "name: \"StepBackQuery\"\n",
    "})\n",
    "const hydeChain = prompt.pipe(llmWithTools);\n",
    "await hydeChain.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
