{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings\n",
    "\n",
    "If we're working with a similarity search-based index, like a vector store, then searching on raw questions may not work well because their embeddings may not be very similar to those of the relevant documents. Instead it might help to have the model generate a hypothetical relevant document, and then use that to perform similarity search. This is the key idea behind [Hypothetical Document Embedding, or HyDE](https://arxiv.org/pdf/2212.10496.pdf).\n",
    "\n",
    "Let's take a look at how we might perform search via hypothetical documents for our Q&A bot over the LangChain YouTube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies\n",
    "\n",
    "```{=mdx}\n",
    "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
    "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
    "\n",
    "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
    "\n",
    "<Npm2Yarn>\n",
    "  @langchain/core @langchain/openai zod\n",
    "</Npm2Yarn>\n",
    "```\n",
    "\n",
    "#### Set environment variables\n",
    "\n",
    "We'll use OpenAI in this example:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your-api-key\n",
    "\n",
    "# Optional, use LangSmith for best-in-class observability\n",
    "LANGSMITH_API_KEY=your-api-key\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b08c52-1ce9-4d8b-a779-cbe8efde51d1",
   "metadata": {},
   "source": [
    "## Hypothetical document generation\n",
    "\n",
    "Ultimately generating a relevant hypothetical document reduces to trying to answer the user question. Since we're desiging a Q&A bot for LangChain YouTube videos, we'll provide some basic context about LangChain and prompt the model to use a more pedantic style so that we get more realistic hypothetical documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\"\n",
    "import { ChatOpenAI } from \"@langchain/openai\"\n",
    "\n",
    "const system = `You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
    "\n",
    "Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question. `;\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "  [\n",
    "  [\"system\", system],\n",
    "  [\"human\", \"{question}\"],\n",
    "  ]\n",
    ")\n",
    "const llm = new ChatOpenAI({\n",
    "    modelName: \"gpt-3.5-turbo-0125\",\n",
    "    temperature: 0\n",
    "})\n",
    "const qaNoContext = prompt.pipe(llm).pipe(new StringOutputParser());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af62af17-4f90-4dbd-a8b4-dfff51f1db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use multi-modal models in a chain and turn the chain into a REST API, you can leverage the capabilities of LangChain, LangGraph, and LangServe. Here's a step-by-step guide on how to achieve this:\n",
      "\n",
      "1. **Set up LangChain**: Start by installing LangChain, LangGraph, and LangServe in your Python environment. You can do this using pip:\n",
      "\n",
      "```bash\n",
      "pip install langchain langgraph langserve\n",
      "```\n",
      "\n",
      "2. **Build a Multi-Modal Model**: Create your multi-modal model using LangChain. LangChain provides integrations with various deep learning frameworks like TensorFlow, PyTorch, and Hugging Face Transformers. You can easily compose different modalities (text, image, audio, etc.) in your model.\n",
      "\n",
      "3. **Use LangGraph for Stateful Multi-Actor Applications**: If your multi-modal model requires stateful interactions between different actors, you can use LangGraph to build such applications. LangGraph simplifies the process of managing state and interactions in your LLM application.\n",
      "\n",
      "4. **Deploy as a REST API using LangServe**: Once you have built your multi-modal model and defined the interactions using LangGraph, you can deploy your chain as a REST API using LangServe. LangServe makes it easy to expose your LangChain application as a web service, allowing users to interact with your model through HTTP requests.\n",
      "\n",
      "5. **Define Endpoints**: In your LangServe application, define the endpoints that correspond to different functionalities of your multi-modal model. For example, you can have endpoints for text input, image input, audio input, etc.\n",
      "\n",
      "6. **Handle Requests**: Implement the logic to handle incoming requests in your LangServe application. Parse the input data, pass it through your multi-modal model, and return the results in the desired format.\n",
      "\n",
      "7. **Start the LangServe Server**: Once you have defined your endpoints and request handling logic, start the LangServe server to make your multi-modal model accessible as a REST API. You can specify the host, port, and other configurations when starting the server.\n",
      "\n",
      "By following these steps, you can effectively use multi-modal models in a chain and expose it as a REST API using LangChain, LangGraph, and LangServe. This approach allows you to build complex LLM applications with stateful interactions and make them accessible to users through a web interface.\n"
     ]
    }
   ],
   "source": [
    "const answer = await qaNoContext.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")\n",
    "console.log(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58a714-9368-4e8e-a163-58bc4a5e56e6",
   "metadata": {},
   "source": [
    "## Returning the hypothetical document and original question\n",
    "\n",
    "To increase our recall we may want to retrieve documents based on both the hypothetical document and the original question. We can easily return both like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447ed63c-ba9f-4eaf-8ed8-b3235e45da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  question: \u001b[32m\"how to use multi-modal models in a chain and turn chain into a rest api\"\u001b[39m,\n",
       "  hypotheticalDocument: \u001b[32m\"To use multi-modal models in a chain and turn the chain into a REST API, you can leverage the capabi\"\u001b[39m... 1920 more characters\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnablePassthrough } from \"@langchain/core/runnables\"\n",
    "\n",
    "\n",
    "const hydeChain = RunnablePassthrough.assign({\n",
    "    hypotheticalDocument: qaNoContext\n",
    "})\n",
    "\n",
    "await hydeChain.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ed79f-cb6f-490e-8186-a4a05b223857",
   "metadata": {},
   "source": [
    "## Using function-calling to get structured output\n",
    "\n",
    "If we were composing this technique with other query analysis techniques, we'd likely be using function calling to get out structured query objects. We can use function-calling for HyDE like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1fecf6-9c07-4efa-80eb-8fb15392b25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  answer: \u001b[32m\"To use multi-modal models in a chain and turn the chain into a REST API, you can follow these steps:\"\u001b[39m... 713 more characters\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { z } from \"zod\"\n",
    "\n",
    "const querySchema = z.object({\n",
    "    answer: z.string().describe(\"Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\")\n",
    "});\n",
    "\n",
    "const system = `You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
    "\n",
    "LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
    "LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
    "LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
    "LangSmith is a platform that makes it easy to trace and test LLM applications.`;\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "    [\n",
    "      [\"system\", system],\n",
    "      [\"human\", \"{question}\"],\n",
    "    ]\n",
    ")\n",
    "const llmWithTools = llm.withStructuredOutput(querySchema, {\n",
    "    name: \"Query\",\n",
    "})\n",
    "const hydeChain = prompt.pipe(llmWithTools);\n",
    "await hydeChain.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
